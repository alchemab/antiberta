{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paratope Prediction using AntiBERTa\n",
    "\n",
    "This notebook describes how one can fine-tune their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and a minimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Sequence,\n",
    "    ClassLabel\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_DIR = \"antibody-tokenizer\"\n",
    "\n",
    "# Initialise a tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(TOKENIZER_DIR, max_len=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing for paratope prediction as a token classification task involves a few steps:\n",
    "* Detecting the actual paratopes from PDB structures (this has already been done for convenience)\n",
    "* Splitting non-redundant sequences (this has already been done for convenience)\n",
    "* Loading them into HuggingFace-compatible `dataset` objects: shown below\n",
    "* Tokenizing the sequences: shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling data into the HF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in parquet files\n",
    "train_df = pd.read_parquet(\n",
    "    'assets/sabdab_train.parquet'\n",
    ")\n",
    "val_df = pd.read_parquet(\n",
    "    'assets/sabdab_val.parquet'\n",
    ")\n",
    "test_df = pd.read_parquet(\n",
    "    'assets/sabdab_test.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>paratope_labels</th>\n",
       "      <th>paratope_sequence</th>\n",
       "      <th>v_gene</th>\n",
       "      <th>j_gene</th>\n",
       "      <th>pdb</th>\n",
       "      <th>antibody_chains</th>\n",
       "      <th>compound</th>\n",
       "      <th>paratope_count</th>\n",
       "      <th>v_gene_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELTQDPAVSVALGQTVRITCQGDSLRSYYASWYQQKPGQAPVLVI...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>IGLV3-19</td>\n",
       "      <td>IGLJ2</td>\n",
       "      <td>6q0e</td>\n",
       "      <td>HL</td>\n",
       "      <td>Inferred precursor (UCA) of the human antibody...</td>\n",
       "      <td>1</td>\n",
       "      <td>VL4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VQLVQSGAAVRKPGASVTVSCKFAEDDDYSPYWVNPAPEHFIHFLR...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>------------------------------P---N--P--F-----...</td>\n",
       "      <td>IGHV1-18</td>\n",
       "      <td>IGHJ1</td>\n",
       "      <td>6nnj</td>\n",
       "      <td>UV</td>\n",
       "      <td>Crystal Structure of HIV-1 BG505 SOSIP.664 Pre...</td>\n",
       "      <td>20</td>\n",
       "      <td>VH8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QVQLQQSGAEVKKPGESLKISCEASGYSFTNYWIGWVRQMPGKGLE...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>--------------------------------W-------------...</td>\n",
       "      <td>IGHV5-51</td>\n",
       "      <td>IGHJ5</td>\n",
       "      <td>5zv3</td>\n",
       "      <td>HL</td>\n",
       "      <td>Crystal structure of human anti-tau antibody C...</td>\n",
       "      <td>10</td>\n",
       "      <td>VH6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  SELTQDPAVSVALGQTVRITCQGDSLRSYYASWYQQKPGQAPVLVI...   \n",
       "1  VQLVQSGAAVRKPGASVTVSCKFAEDDDYSPYWVNPAPEHFIHFLR...   \n",
       "2  QVQLQQSGAEVKKPGESLKISCEASGYSFTNYWIGWVRQMPGKGLE...   \n",
       "\n",
       "                                     paratope_labels  \\\n",
       "0  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "1  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "2  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "\n",
       "                                   paratope_sequence    v_gene j_gene   pdb  \\\n",
       "0  ----------------------------------------------...  IGLV3-19  IGLJ2  6q0e   \n",
       "1  ------------------------------P---N--P--F-----...  IGHV1-18  IGHJ1  6nnj   \n",
       "2  --------------------------------W-------------...  IGHV5-51  IGHJ5  5zv3   \n",
       "\n",
       "  antibody_chains                                           compound  \\\n",
       "0              HL  Inferred precursor (UCA) of the human antibody...   \n",
       "1              UV  Crystal Structure of HIV-1 BG505 SOSIP.664 Pre...   \n",
       "2              HL  Crystal structure of human anti-tau antibody C...   \n",
       "\n",
       "   paratope_count v_gene_cluster  \n",
       "0               1            VL4  \n",
       "1              20            VH8  \n",
       "2              10            VH6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dataset Dict with the sequence and paratope labels\n",
    "ab_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[['sequence','paratope_labels']]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[['sequence','paratope_labels']]),\n",
    "    \"test\": Dataset.from_pandas(test_df[['sequence','paratope_labels']])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'paratope_labels'],\n",
       "        num_rows: 720\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'paratope_labels'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'paratope_labels'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what a DatasetDict object looks like with its individual Dataset things\n",
    "ab_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELTQDPAVSVALGQTVRITCQGDSLRSYYASWYQQKPGQAPVLVIYGKNNRPSGIPDRFSGSSSGNTASLTITGAQAEDEADYYCNSRDSSGNHPVVFGGGTKLTVL']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset['train'].select(range(1))['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(ab_dataset['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the Features of each column in the train dataset within the ab_dataset DatasetDict\n",
    "ab_dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the `paratope_labels` column into a set of `ClassLabel`s, which will be predicted via the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ClassLabel feature which will replace paratope_labels later.\n",
    "paratope_class_label = ClassLabel(2, names=['N','P'])\n",
    "new_feature = Sequence(\n",
    "    paratope_class_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709e0cd5057d4066b5dee8b517c84433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da65f89fcaf7443d984d4ae1d67332d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b76b7b897d46a7979674af29be497f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We iterate through the sequence and labels columns\n",
    "# Keeping the sequence column as-is, but applying a str2int function, allowing us to cast later\n",
    "ab_dataset_featurised = ab_dataset.map(\n",
    "    lambda seq, labels: {\n",
    "        \"sequence\": seq,\n",
    "        \"paratope_labels\": [paratope_class_label.str2int(sample) for sample in labels]\n",
    "    }, \n",
    "    input_columns=[\"sequence\", \"paratope_labels\"], batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the old Features instance from the previous ab_dataset\n",
    "# Notice how labels is a Sequence of Value\n",
    "feature_set_copy = ab_dataset['train'].features.copy()\n",
    "feature_set_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to the `new_feature` that we made earlier\n",
    "feature_set_copy['paratope_labels'] = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056b0aea556e454887d2bcb8a43f4987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0b2a6e1edf43269f1cf6a48ca32dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc54d47da1a4e588a0f8f18a526545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ab_dataset_featurised = ab_dataset_featurised.cast(feature_set_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=ClassLabel(num_classes=2, names=['N', 'P'], names_file=None, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset_featurised['train'].features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# now the labels are actually a series of integers, but is recognised by huggingface as a series of Classlabels\n",
    "print(ab_dataset_featurised['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to redefine this to e.g. put -100 labels for the start/end tokens\n",
    "\n",
    "def preprocess(batch):\n",
    "    # :facepalm: The preprocess function takes tokenizer and needs a LIST not a PT tensor :eyeroll:\n",
    "    # https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=vc0BSBLIIrJQ\n",
    "    \n",
    "    t_inputs = tokenizer(batch['sequence'], \n",
    "        padding=\"max_length\")\n",
    "    batch['input_ids'] = t_inputs.input_ids\n",
    "    batch['attention_mask'] = t_inputs.attention_mask\n",
    "    \n",
    "    # enumerate \n",
    "    labels_container = []\n",
    "    for index, labels in enumerate(batch['paratope_labels']):\n",
    "        \n",
    "        # This is typically length of the sequence + SOS + EOS + PAD (to longest example in batch)\n",
    "        tokenized_input_length = len(batch['input_ids'][index])\n",
    "        paratope_label_length  = len(batch['paratope_labels'][index])\n",
    "        \n",
    "        # we subtract 1 because we start with SOS\n",
    "        # we should in theory have at least 1 \"pad_with_eos\" because an EOS wouldn't have been accounted for in the\n",
    "        # paratope_labels column even for the longest possible sequence\n",
    "        n_pads_with_eos = max(1, tokenized_input_length - paratope_label_length - 1)\n",
    "        \n",
    "        # We have a starting -100 for the SOS\n",
    "        # and fill the rest of seq length with -100 to account for any extra pads and the final EOS token\n",
    "        # The -100s are ignored in the CE loss function\n",
    "        labels_padded = [-100] + labels + [-100] * n_pads_with_eos\n",
    "        \n",
    "        assert len(labels_padded) == len(batch['input_ids'][index]), \\\n",
    "        f\"Lengths don't align, {len(labels_padded)}, {len(batch['input_ids'][index])}, {len(labels)}\"\n",
    "        \n",
    "        labels_container.append(labels_padded)\n",
    "    \n",
    "    # We create a new column called `labels`, which is recognised by the HF trainer object\n",
    "    batch['labels'] = labels_container\n",
    "    \n",
    "    for i,v in enumerate(batch['labels']):\n",
    "        assert len(batch['input_ids'][i]) == len(v) == len(batch['attention_mask'][i])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c8ff5eee08426ba9f067721b2adeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d17a161c4964975bc54ea05f469c04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0130e72d0e40adb28c015712a4d96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply that function above on the dataset - labels now aligned!\n",
    "ab_dataset_tokenized = ab_dataset_featurised.map(\n",
    "    preprocess, \n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=['sequence', 'paratope_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model set-up and training\n",
    "\n",
    "Here we define:\n",
    "* The callback function to compute some metrics during training (and can be used for evaluation!)\n",
    "* The training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has the actual names that maps 0->N and 1->P\n",
    "label_list = paratope_class_label.names\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    A callback added to the trainer so that we calculate various metrics via sklearn\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # The predictions are logits, so we apply softmax to get the probabilities. We only need\n",
    "    # the probabilities of the paratope label, which is index 1 (according to the ClassLabel we made earlier),\n",
    "    # or the last column from the output tensor\n",
    "    prediction_pr = torch.softmax(torch.from_numpy(predictions), dim=2).detach().numpy()[:,:,-1]\n",
    "    \n",
    "    # We run an argmax to get the label\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Only compute on positions that are not labelled -100\n",
    "    preds = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    labs = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    probs = [ \n",
    "        [prediction_pr[i][pos] for (pr, (pos, l)) in zip(prediction, enumerate(label)) if l!=-100]\n",
    "         for i, (prediction, label) in enumerate(zip(predictions, labels)) \n",
    "    ] \n",
    "            \n",
    "    # flatten\n",
    "    preds = sum(preds, [])\n",
    "    labs = sum(labs, [])\n",
    "    probs = sum(probs,[])\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(labs, preds, pos_label=\"P\"),\n",
    "        \"recall\": recall_score(labs, preds, pos_label=\"P\"),\n",
    "        \"f1\": f1_score(labs, preds, pos_label=\"P\"),\n",
    "        \"auc\": roc_auc_score(labs, probs),\n",
    "        \"aupr\": average_precision_score(labs, probs, pos_label=\"P\"),\n",
    "        \"mcc\": matthews_corrcoef(labs, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size, metric you want etc. \n",
    "batch_size = 32\n",
    "RUN_ID = \"paratope-prediction-task\"\n",
    "SEED = 0\n",
    "LR = 1e-6\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{RUN_ID}_{SEED}\", # this is the name of the checkpoint folder\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR, # 1e-6, 5e-6, 1e-5. .... 1e-3\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0, # 0, 0.05, 0.1 .... \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    metric_for_best_model='aupr', # name of the metric here should correspond to metrics defined in compute_metrics\n",
    "    logging_strategy='epoch',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set all seeds to make results reproducible (deterministic mode).\n",
    "    When seed is None, disables deterministic mode.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Name of the pre-trained model after you train your MLM\n",
    "MODEL_DIR = \"antiberta-base\"\n",
    "\n",
    "# We initialise a model using the weights from the pre-trained model\n",
    "model = RobertaForTokenClassification.from_pretrained(MODEL_DIR, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ab_dataset_tokenized['train'],\n",
    "    eval_dataset=ab_dataset_tokenized['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch stuff fly\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction on the test set\n",
    "pred = trainer.predict(\n",
    "    ab_dataset_tokenized['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stores a JSON with metric values\n",
    "pred.metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - how to go from sequence to predicted sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sequence of tralokinumab Light chain\n",
    "input_seq = 'YVLTQPPSVSVAPGKTARITCGGNIIGSKLVHWYQQKPGQAPVLVIYDDGDRPSGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDTGSDPVVFGGGTKLTVL'\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    f\"{RUN_ID}_{SEED}\"\n",
    ")\n",
    "\n",
    "tokenized_input = tokenizer([input_seq], return_tensors='pt', padding=True)\n",
    "predicted_logits = model(**tokenized_input)\n",
    "\n",
    "# Simple argmax - no thresholding.\n",
    "argmax = predicted_logits[0].argmax(2)[0][1:-1].cpu().numpy()\n",
    "indices = np.argwhere(argmax).flatten()\n",
    "\n",
    "predicted_sequence = ''\n",
    "\n",
    "for i, s in enumerate(input_seq):\n",
    "    if i in indices:\n",
    "        predicted_sequence += s\n",
    "    else:\n",
    "        predicted_sequence += '-'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
